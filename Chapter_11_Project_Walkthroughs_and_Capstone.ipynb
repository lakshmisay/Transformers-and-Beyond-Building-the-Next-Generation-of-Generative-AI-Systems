{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1229e4ff",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 11: Project Walkthroughs and Capstone\n",
    "\n",
    "This notebook provides hands-on walkthroughs of real-world Generative AI projects, including:\n",
    "- A Retrieval-Augmented Generation (RAG) pipeline\n",
    "- A conversational chatbot with LangChain\n",
    "- A multimodal application combining text and vision\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Combine components from previous chapters into complete applications\n",
    "- Implement a functional GenAI-powered chatbot\n",
    "- Build and test a RAG pipeline using LangChain + FAISS\n",
    "- Explore multimodal inputs using CLIP or BLIP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b5789f",
   "metadata": {},
   "source": [
    "\n",
    "## Project 1: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "We’ll build a simple RAG system using Hugging Face + LangChain + FAISS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4fe0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Sample data\n",
    "texts = [\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"Mount Everest is the tallest mountain in the world.\",\n",
    "    \"The Great Wall of China is visible from space.\"\n",
    "]\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(), retriever=retriever)\n",
    "result = qa_chain.run(\"Where is the Eiffel Tower?\")\n",
    "print(\"RAG Output:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982c33f",
   "metadata": {},
   "source": [
    "\n",
    "## Project 2: LangChain Chatbot with Memory\n",
    "\n",
    "This chatbot remembers previous conversation turns using buffer memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "chatbot = ConversationChain(llm=OpenAI(), memory=memory)\n",
    "\n",
    "chatbot.predict(input=\"Hi, I am Alice.\")\n",
    "chatbot.predict(input=\"What is my name?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec3349",
   "metadata": {},
   "source": [
    "\n",
    "## Project 3: Multimodal Image Captioning with BLIP\n",
    "\n",
    "We'll use Salesforce BLIP to caption an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(requests.get(\"https://raw.githubusercontent.com/salesforce/BLIP/main/demo.jpg\", stream=True).raw)\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs)\n",
    "caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd519c",
   "metadata": {},
   "source": [
    "\n",
    "## Capstone Project Suggestions\n",
    "\n",
    "- AI Tutor: Chat + Retrieval + Grading\n",
    "- Legal Assistant: Document retrieval + summarization + sentiment\n",
    "- Medical Bot: Symptom checker + recommendation\n",
    "- AR/VR: Visual captioning + prompt-based 3D narration\n",
    "\n",
    "## Deployment Options\n",
    "\n",
    "- Hugging Face Spaces\n",
    "- Streamlit + FastAPI + Docker\n",
    "- LangServe or Gradio for frontend\n",
    "\n",
    "Plan your capstone in stages: data → pipeline → test → deploy → monitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc00ad",
   "metadata": {},
   "source": [
    "\n",
    "## Exercises\n",
    "\n",
    "1. Add more documents to the RAG system and test with new questions.\n",
    "2. Customize the chatbot to include tool access or role-play personalities.\n",
    "3. Extend BLIP with visual question answering using BLIP-2.\n",
    "4. Deploy one complete project using Streamlit or Docker.\n",
    "\n",
    "## References\n",
    "\n",
    "- LangChain: https://docs.langchain.com\n",
    "- Hugging Face: https://huggingface.co\n",
    "- BLIP Model: https://github.com/salesforce/BLIP\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
