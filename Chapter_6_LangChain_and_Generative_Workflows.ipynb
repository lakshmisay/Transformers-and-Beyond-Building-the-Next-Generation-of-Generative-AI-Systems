{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc0cc9a",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 6: LangChain and Generative Workflows\n",
    "\n",
    "This notebook covers:\n",
    "- LangChain architecture: Chains, Agents, Memory\n",
    "- How to use document loaders and text splitters\n",
    "- Connecting retrievers and vector stores\n",
    "- Designing multi-step workflows with LangChain\n",
    "- Deploying via LangServe\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the components of LangChain: Prompt, LLM, Chain, Agent\n",
    "- Load and split documents for context-aware processing\n",
    "- Connect vector databases like FAISS with LangChain retrievers\n",
    "- Build multi-step chains and agent tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34eb3bb",
   "metadata": {},
   "source": [
    "\n",
    "## LangChain Components Overview\n",
    "\n",
    "| Component | Purpose |\n",
    "|----------|---------|\n",
    "| PromptTemplate | Parameterized prompt templates |\n",
    "| LLMChain | Chains that use LLMs |\n",
    "| Memory | Store interaction history |\n",
    "| Retriever | Fetch context from knowledge base |\n",
    "| Agent | Execute reasoning with tools |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18501b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI  # Replace with local model if needed\n",
    "\n",
    "# Create a prompt template\n",
    "template = PromptTemplate.from_template(\"Translate the following to Spanish: {text}\")\n",
    "llm = OpenAI(temperature=0.7)  # Requires API key\n",
    "chain = LLMChain(prompt=template, llm=llm)\n",
    "\n",
    "# Run the chain\n",
    "output = chain.run(\"Good morning, how are you?\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d540ba8",
   "metadata": {},
   "source": [
    "\n",
    "## Document Loading and Splitting\n",
    "\n",
    "LangChain supports loaders (PDF, web, TXT) and splitters for chunking text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da0653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(file_path=\"sample.txt\")  # Replace with a valid file path\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "print(f\"Loaded {len(split_docs)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ce3a93",
   "metadata": {},
   "source": [
    "\n",
    "## Retriever + VectorStore Integration\n",
    "\n",
    "You can embed documents and retrieve them for query answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b79820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "texts = [\"LangChain enables generative workflows.\", \"Retrievers fetch relevant information for answering.\"]\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "db = FAISS.from_texts(texts, embeddings)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "query = \"What is LangChain?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(\"Top document retrieved:\", docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3761592",
   "metadata": {},
   "source": [
    "\n",
    "## Multi-Step Generative Workflow\n",
    "\n",
    "Example: Retrieve → Summarize → Refine Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# First: Summarize\n",
    "summarize_prompt = PromptTemplate.from_template(\"Summarize: {text}\")\n",
    "summarize_chain = LLMChain(llm=llm, prompt=summarize_prompt)\n",
    "\n",
    "# Second: Rephrase\n",
    "refine_prompt = PromptTemplate.from_template(\"Rephrase more formally: {text}\")\n",
    "refine_chain = LLMChain(llm=llm, prompt=refine_prompt)\n",
    "\n",
    "# Combine\n",
    "workflow = SimpleSequentialChain(chains=[summarize_chain, refine_chain])\n",
    "result = workflow.run(\"LangChain is a cool framework to connect LLMs.\")\n",
    "print(\"Final output:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08091e7",
   "metadata": {},
   "source": [
    "\n",
    "## Deploying with LangServe\n",
    "\n",
    "LangChain supports deployment of chains using LangServe + FastAPI.\n",
    "\n",
    "```bash\n",
    "pip install langserve\n",
    "```\n",
    "\n",
    "```python\n",
    "from langserve import add_routes\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "add_routes(app, chain=your_chain, path=\"/generate\")\n",
    "```\n",
    "\n",
    "Then launch with `uvicorn main:app --reload`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ac3e6b",
   "metadata": {},
   "source": [
    "\n",
    "## Exercises\n",
    "\n",
    "1. Load a Wikipedia article and chunk it using LangChain.\n",
    "2. Index your documents in FAISS and build a custom retriever.\n",
    "3. Create a multi-step workflow using 3 chains.\n",
    "4. Use LangServe to expose a chatbot chain as an API.\n",
    "\n",
    "## References\n",
    "\n",
    "- LangChain Docs: https://docs.langchain.com\n",
    "- LangServe: https://github.com/langchain-ai/langserve\n",
    "- FAISS: https://github.com/facebookresearch/faiss\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
