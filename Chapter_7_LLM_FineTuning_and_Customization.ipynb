{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e16d3cf",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 7: LLM Fine-Tuning and Customization\n",
    "\n",
    "This notebook covers:\n",
    "- Strategies for fine-tuning language models\n",
    "- PEFT methods: LoRA and QLoRA\n",
    "- Dataset formatting for supervised fine-tuning (SFT)\n",
    "- Using Hugging Face's Trainer for fine-tuning\n",
    "- Evaluating fine-tuned models\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the differences between full fine-tuning and PEFT\n",
    "- Prepare prompt-response dataset for instruction tuning\n",
    "- Fine-tune a model using LoRA with Hugging Face\n",
    "- Evaluate generation quality and loss metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a46ea1",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset Preparation for Supervised Fine-Tuning (SFT)\n",
    "\n",
    "The dataset should be in a prompt-response format, typically as JSON or CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de924f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_data = [\n",
    "    {\"prompt\": \"What is generative AI?\", \"response\": \"Generative AI refers to models that create new content such as text, images, or code.\"},\n",
    "    {\"prompt\": \"Explain LoRA.\", \"response\": \"LoRA is a parameter-efficient fine-tuning method that adds trainable low-rank matrices to attention layers.\"}\n",
    "]\n",
    "\n",
    "import json\n",
    "with open(\"sft_data.json\", \"w\") as f:\n",
    "    json.dump(sample_data, f, indent=2)\n",
    "\n",
    "print(\"Sample dataset saved as sft_data.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a427c",
   "metadata": {},
   "source": [
    "\n",
    "## Fine-Tuning using Hugging Face Trainer\n",
    "\n",
    "We load the model and tokenizer, prepare the dataset, define training args, and launch training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ccc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Load data (simplified)\n",
    "from datasets import Dataset\n",
    "data = Dataset.from_list(sample_data)\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(example):\n",
    "    return tokenizer(f\"### Prompt: {example['prompt']}\\n### Response: {example['response']}\", truncation=True)\n",
    "\n",
    "tokenized_data = data.map(tokenize)\n",
    "\n",
    "# Setup training\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "trainer = Trainer(model=model, args=args, train_dataset=tokenized_data, data_collator=data_collator)\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1657871",
   "metadata": {},
   "source": [
    "\n",
    "## LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA adapts only low-rank matrices in attention layers. Useful for large model tuning on small hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cdfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec1c2a",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "Use metrics like:\n",
    "- Perplexity\n",
    "- BLEU, ROUGE for text outputs\n",
    "- Custom scoring functions for relevance and fluency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe87ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"What is generative AI?\"\n",
    "input_ids = tokenizer(f\"### Prompt: {prompt}\\n### Response:\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304504a",
   "metadata": {},
   "source": [
    "\n",
    "## Exercises\n",
    "\n",
    "1. Convert your dataset into JSON format for instruction tuning.\n",
    "2. Try LoRA fine-tuning using your custom dataset.\n",
    "3. Compare full fine-tuning vs LoRA in terms of speed and accuracy.\n",
    "4. Evaluate generated answers using BLEU or ROUGE.\n",
    "\n",
    "## References\n",
    "\n",
    "- PEFT: https://github.com/huggingface/peft\n",
    "- Hugging Face Trainer: https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "- LoRA Paper: https://arxiv.org/abs/2106.09685\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
