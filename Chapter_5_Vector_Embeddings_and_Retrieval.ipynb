{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f19858",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 5: Vector Embeddings and Retrieval\n",
    "\n",
    "This notebook explores:\n",
    "- What are embeddings and how they work\n",
    "- How to generate embeddings using pre-trained models\n",
    "- Vector databases and similarity search\n",
    "- Retrieval-Augmented Generation (RAG) pipelines\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Generate vector embeddings from text using Hugging Face or Sentence Transformers\n",
    "- Use cosine similarity for semantic comparison\n",
    "- Index and retrieve using FAISS and ChromaDB\n",
    "- Build a basic Retrieval-Augmented Generation (RAG) pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a8896",
   "metadata": {},
   "source": [
    "\n",
    "## Generating Text Embeddings\n",
    "\n",
    "We'll use SentenceTransformers to generate embeddings from a list of sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "sentences = [\"AI is transforming the world.\", \"Artificial Intelligence is the future.\", \"Bananas are yellow.\"]\n",
    "\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "print(\"Embedding shape:\", embeddings.shape)\n",
    "\n",
    "# Cosine similarity\n",
    "cos_sim = util.pytorch_cos_sim(embeddings[0], embeddings)\n",
    "print(\"Cosine similarity matrix:\\n\", cos_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf0d33",
   "metadata": {},
   "source": [
    "\n",
    "## Similarity Search using FAISS\n",
    "\n",
    "FAISS is a library for efficient similarity search and clustering of dense vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a367e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Convert to numpy\n",
    "emb_np = embeddings.cpu().detach().numpy().astype('float32')\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(emb_np.shape[1])\n",
    "index.add(emb_np)\n",
    "\n",
    "query = model.encode([\"AI innovations\"], convert_to_tensor=True).cpu().detach().numpy().astype('float32')\n",
    "D, I = index.search(query, k=2)\n",
    "\n",
    "print(\"Most similar sentences:\")\n",
    "for idx in I[0]:\n",
    "    print(\"-\", sentences[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8435717",
   "metadata": {},
   "source": [
    "\n",
    "## Using ChromaDB for Persistent Storage\n",
    "\n",
    "ChromaDB supports storing and querying documents with metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551fca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "chroma_client = chromadb.Client(Settings(allow_reset=True))\n",
    "chroma_client.reset()  # clear previous\n",
    "\n",
    "collection = chroma_client.create_collection(name=\"genai_docs\")\n",
    "collection.add(\n",
    "    documents=[\"Generative AI is powerful.\", \"LangChain helps chain LLM calls.\"],\n",
    "    ids=[\"doc1\", \"doc2\"],\n",
    "    metadatas=[{\"type\": \"intro\"}, {\"type\": \"tool\"}]\n",
    ")\n",
    "\n",
    "results = collection.query(query_texts=[\"What is LangChain?\"], n_results=1)\n",
    "print(\"Top result:\", results[\"documents\"][0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab76b2f",
   "metadata": {},
   "source": [
    "\n",
    "## Retriever-Augmented Generation (RAG)\n",
    "\n",
    "Use retrieved documents to answer a query using a language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Build vector DB\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "texts = [\"Vector databases power semantic search.\", \"FAISS is fast and memory efficient.\"]\n",
    "db = FAISS.from_texts(texts, embedding_model)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "# Uncomment below line if using OpenAI or LangChain-compatible model\n",
    "# qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(), retriever=retriever)\n",
    "# print(qa_chain.run(\"What is FAISS used for?\"))\n",
    "print(\"RAG setup done â€” replace OpenAI with a local model for full pipeline.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750bb26",
   "metadata": {},
   "source": [
    "\n",
    "## Exercises\n",
    "\n",
    "1. Add more documents and test different queries using FAISS and ChromaDB.\n",
    "2. Try using cosine similarity instead of L2 distance for FAISS.\n",
    "3. Integrate OpenAI, Cohere, or Hugging Face models into the RAG pipeline.\n",
    "4. Visualize embedding space using t-SNE or PCA.\n",
    "\n",
    "## References\n",
    "\n",
    "- Sentence Transformers: https://www.sbert.net\n",
    "- FAISS: https://github.com/facebookresearch/faiss\n",
    "- ChromaDB: https://docs.trychroma.com\n",
    "- LangChain RAG: https://docs.langchain.com/docs/use_cases/question_answering\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
