{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa8e68d",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 2: Core Architectures and Training\n",
    "\n",
    "This notebook covers fundamental generative architectures including:\n",
    "- Autoencoders (AEs)\n",
    "- Variational Autoencoders (VAEs)\n",
    "- Generative Adversarial Networks (GANs)\n",
    "- Transformers and Attention Mechanisms\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement autoencoders and VAEs in PyTorch\n",
    "- Understand GAN architecture and training loop\n",
    "- Explore transformer structure: encoder, decoder, and attention\n",
    "- Use self-attention and multi-head attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122628c",
   "metadata": {},
   "source": [
    "\n",
    "## Autoencoders\n",
    "\n",
    "Autoencoders compress data into a latent space and reconstruct the original input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e18bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=128):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0e5b7",
   "metadata": {},
   "source": [
    "\n",
    "## Variational Autoencoders (VAEs)\n",
    "\n",
    "VAEs encode the input into a probabilistic latent space and allow for sampling from the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, latent_dim=2):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, 400), nn.ReLU())\n",
    "        self.fc_mu = nn.Linear(400, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(400, latent_dim)\n",
    "        self.decoder = nn.Sequential(nn.Linear(latent_dim, 400), nn.ReLU(), nn.Linear(400, input_dim), nn.Sigmoid())\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.encoder(x)\n",
    "        mu, logvar = self.fc_mu(h1), self.fc_logvar(h1)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def vae_loss(reconstructed, original, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(reconstructed, original, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64916024",
   "metadata": {},
   "source": [
    "\n",
    "## Generative Adversarial Networks (GANs)\n",
    "\n",
    "GANs consist of two neural networks:\n",
    "- Generator: produces fake data\n",
    "- Discriminator: classifies real vs. fake data\n",
    "\n",
    "They train through adversarial loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfbc757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=100, output_dim=784):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(), nn.Linear(256, output_dim), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=784):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.LeakyReLU(0.2), nn.Linear(256, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b71cf",
   "metadata": {},
   "source": [
    "\n",
    "## Transformers and Attention\n",
    "\n",
    "Transformers rely on **self-attention** mechanisms and are widely used in NLP and multimodal generation.\n",
    "\n",
    "A minimal transformer model includes:\n",
    "- Positional encoding\n",
    "- Multi-head attention\n",
    "- Feed-forward layers\n",
    "\n",
    "We'll use Hugging Face Transformers to demonstrate text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94631de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "input_text = \"The transformer architecture is\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=40)\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac9f90",
   "metadata": {},
   "source": [
    "\n",
    "## Attention Calculation (Self-Attention)\n",
    "\n",
    "The attention score between tokens is computed using the formula:\n",
    "\n",
    "\\[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- Q = Queries\n",
    "- K = Keys\n",
    "- V = Values\n",
    "- \\( d_k \\) = dimension of key vectors\n",
    "\n",
    "Use this in a transformer for capturing context between tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1046ec",
   "metadata": {},
   "source": [
    "\n",
    "## Exercises\n",
    "\n",
    "1. Implement a training loop for VAE using MNIST dataset.\n",
    "2. Implement the GAN training loop and generate digits.\n",
    "3. Explore encoder-decoder attention using a translation model.\n",
    "4. Visualize attention weights in a transformer layer.\n",
    "\n",
    "## References\n",
    "\n",
    "- PyTorch VAE Example: https://github.com/pytorch/examples/tree/main/vae\n",
    "- DCGAN Tutorial: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "- Hugging Face Transformers: https://huggingface.co/docs/transformers\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
